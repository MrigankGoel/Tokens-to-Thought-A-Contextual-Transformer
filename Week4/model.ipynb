{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path: str) -> str:\n",
    "    \"\"\"Load raw text and do minimal preprocessing.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read().replace(\"\\n\", \" \")\n",
    "    return data\n",
    "\n",
    "data = load_text(\"training_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data: str):\n",
    "    \"\"\"\n",
    "    Build character set + integer encodings.\n",
    "\n",
    "    Convention (same spirit as your original):\n",
    "    - Reserve 0 optionally for padding/unknown (you decide).\n",
    "    - Map characters to 1..V-1.\n",
    "    \"\"\"\n",
    "    characters = list(set(list(data)))\n",
    "    vocab_size = len(characters) + 1\n",
    "\n",
    "    char2idx = {}\n",
    "    idx2char = {}\n",
    "    for i, ch in enumerate(characters):\n",
    "        char2idx[ch] = i + 1\n",
    "        idx2char[i + 1] = ch\n",
    "\n",
    "    return characters, vocab_size, char2idx, idx2char\n",
    "\n",
    "characters, input_vocab_size, character_to_integer_encoding, integer_to_character_encoding = build_vocab(data)\n",
    "print(len(characters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(string: str):\n",
    "    \"\"\"Convert string -> list of token ids.\"\"\"\n",
    "    global character_to_integer_encoding\n",
    "    return [character_to_integer_encoding[ch] for ch in string]\n",
    "\n",
    "def decode(lst):\n",
    "    \"\"\"Convert list of token ids -> string.\"\"\"\n",
    "    global integer_to_character_encoding\n",
    "    return \"\".join(integer_to_character_encoding[i] for i in lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = encode(data)\n",
    "train_data = input_data[:int(0.9 * len(input_data))]\n",
    "test_data  = input_data[int(0.9 * len(input_data)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "block_size = 128\n",
    "num_heads = 8\n",
    "num_transformer_blocks = 4\n",
    "embed_dim = 256\n",
    "feed_forward_dim = 256\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src):\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) attention mask so position i cannot attend to j > i.\n",
    "\n",
    "    TODO:\n",
    "    - Build a boolean mask of shape (batch_size, n_dest, n_src) (or broadcastable equivalent).\n",
    "    - Ensure compatibility with tf.keras.layers.MultiHeadAttention(attention_mask=...).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement causal_attention_mask\")\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    One transformer block:\n",
    "    - Causal self-attention\n",
    "    - Residual + LayerNorm\n",
    "    - Feed-forward MLP\n",
    "    - Residual + LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: define attention layer\n",
    "        # e.g., layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "        self.att = None\n",
    "\n",
    "        # TODO: define feed-forward network\n",
    "        # e.g., tf.keras.Sequential([Dense(ff_dim, activation=...), Dense(embed_dim)])\n",
    "        self.ffn = None\n",
    "\n",
    "        self.normalization_layer_1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.normalization_layer_2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (batch, seq_len, embed_dim)\n",
    "\n",
    "        TODO:\n",
    "        - Build causal mask\n",
    "        - Run self-attention with that mask\n",
    "        - Residual + norm\n",
    "        - Feed-forward\n",
    "        - Residual + norm\n",
    "        - Return output\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"TODO: implement TransformerBlock.call\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"Embeds tokens + positions and adds them.\"\"\"\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_embedding = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len)\n",
    "\n",
    "        TODO:\n",
    "        - Create positions = tf.range(0, seq_len)\n",
    "        - pos_emb = self.pos_embedding(positions)\n",
    "        - tok_emb = self.token_embedding(x)\n",
    "        - Return tok_emb + pos_emb (broadcast over batch)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"TODO: implement TokenAndPositionEmbedding.call\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_model(\n",
    "    maxlen,\n",
    "    vocab_size,\n",
    "    embed_dim,\n",
    "    num_heads,\n",
    "    feed_forward_dim,\n",
    "    num_transformer_blocks=1,\n",
    "    rate=0.1,\n",
    "):\n",
    "    \"\"\"Functional API model builder for causal next-token prediction.\"\"\"\n",
    "    inputs = Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    x = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)(inputs)\n",
    "\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, feed_forward_dim, rate=rate)(x)\n",
    "\n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    model = Model(inputs=inputs, outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_transformer_model(block_size, input_vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks, rate=dropout_rate)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=[loss_fn],\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_next_token_dataset(token_ids, block_size):\n",
    "    \"\"\"\n",
    "    inputs[i]  = token_ids[i : i+block_size]\n",
    "    targets[i] = token_ids[i+1 : i+block_size+1]\n",
    "    \"\"\"\n",
    "    inputs = [token_ids[i:i+block_size] for i in range(0, len(token_ids) - block_size - 1)]\n",
    "    targets = [token_ids[i+1:i+block_size+1] for i in range(0, len(token_ids) - block_size - 1)]\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def build_tf_dataset(inputs, targets, batch_size, shuffle_buffer=10000):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    - Convert inputs/targets to np.array or tf.Tensor with dtype int32\n",
    "    - dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    - dataset = dataset.shuffle(shuffle_buffer)\n",
    "    - dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    - return dataset\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"TODO: implement build_tf_dataset\")\n",
    "\n",
    "\n",
    "inputs, targets = make_next_token_dataset(train_data, block_size)\n",
    "\n",
    "# TODO: after implementing build_tf_dataset, create dataset like this:\n",
    "# dataset = build_tf_dataset(inputs, targets, batch_size=batch_size, shuffle_buffer=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training cell (intent
